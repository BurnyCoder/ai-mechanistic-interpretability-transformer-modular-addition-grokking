{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eH_gYLwQXXw1"
   },
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/TransformerLensOrg/TransformerLens/blob/main/demos/Grokking_Demo.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EnMLgl6CXXw5"
   },
   "source": [
    "# Grokking Demo Notebook\n",
    "\n",
    "<b style=\"color: red\">To use this notebook, go to Runtime > Change Runtime Type and select GPU as the hardware accelerator.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZXzbNWp1XXw5"
   },
   "source": [
    "# Setup\n",
    "(No need to read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "YT9A7qImXXw6"
   },
   "outputs": [],
   "source": [
    "TRAIN_MODEL = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "1QAAikXFXXw7",
    "outputId": "e69cbf4d-681c-4cf3-cfad-0504d82dae85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running as a Jupyter notebook - intended for development only!\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Janky code to do different setup when run in a Colab notebook vs VSCode\n",
    "import os\n",
    "\n",
    "DEVELOPMENT_MODE = True\n",
    "IN_GITHUB = os.getenv(\"GITHUB_ACTIONS\") == \"true\"\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"Running as a Colab notebook\")\n",
    "\n",
    "    # PySvelte is an unmaintained visualization library, use it as a backup if circuitsvis isn't working\n",
    "    # # Install another version of node that makes PySvelte work way faster\n",
    "    # !curl -fsSL https://deb.nodesource.com/setup_16.x | sudo -E bash -; sudo apt-get install -y nodejs\n",
    "    # %pip install git+https://github.com/neelnanda-io/PySvelte.git\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    print(\"Running as a Jupyter notebook - intended for development only!\")\n",
    "    from IPython import get_ipython\n",
    "\n",
    "    ipython = get_ipython()\n",
    "    # Code to automatically update the HookedTransformer code as its edited without restarting the kernel\n",
    "    ipython.run_line_magic(\"load_ext\", \"autoreload\")\n",
    "    ipython.run_line_magic(\"autoreload\", \"2\")\n",
    "\n",
    "if IN_COLAB or IN_GITHUB:\n",
    "    %pip install transformer_lens\n",
    "    %pip install circuitsvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "4O5mJ3XWXXw8",
    "outputId": "b2005ce7-8e37-472c-bcc1-2ffbf20dbbcf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using renderer: notebook_connected\n"
     ]
    }
   ],
   "source": [
    "# Plotly needs a different renderer for VSCode/Notebooks vs Colab argh\n",
    "import plotly.io as pio\n",
    "if IN_COLAB or not DEVELOPMENT_MODE:\n",
    "    pio.renderers.default = \"colab\"\n",
    "else:\n",
    "    pio.renderers.default = \"notebook_connected\"\n",
    "print(f\"Using renderer: {pio.renderers.default}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "FLJVEwzNXXw8"
   },
   "outputs": [],
   "source": [
    "pio.templates['plotly'].layout.xaxis.title.font.size = 20\n",
    "pio.templates['plotly'].layout.yaxis.title.font.size = 20\n",
    "pio.templates['plotly'].layout.title.font.size = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "N9G0SXPnXXw8"
   },
   "outputs": [],
   "source": [
    "# Import stuff\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import einops\n",
    "from fancy_einsum import einsum\n",
    "import os\n",
    "import tqdm.auto as tqdm\n",
    "import random\n",
    "from pathlib import Path\n",
    "import plotly.express as px\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from typing import List, Union, Optional\n",
    "from functools import partial\n",
    "import copy\n",
    "\n",
    "import itertools\n",
    "from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer\n",
    "import dataclasses\n",
    "import datasets\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "Qo3Mj7HQXXw9"
   },
   "outputs": [],
   "source": [
    "import transformer_lens\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens.hook_points import (\n",
    "    HookedRootModule,\n",
    "    HookPoint,\n",
    ")  # Hooking utilities\n",
    "from transformer_lens import HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UHwqiiMwXXw9"
   },
   "source": [
    "Plotting helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "QlgyB6nnXXw-"
   },
   "outputs": [],
   "source": [
    "def imshow(tensor, renderer=None, xaxis=\"\", yaxis=\"\", **kwargs):\n",
    "    px.imshow(utils.to_numpy(tensor), color_continuous_midpoint=0.0, color_continuous_scale=\"RdBu\", labels={\"x\":xaxis, \"y\":yaxis}, **kwargs).show(renderer)\n",
    "\n",
    "def line(tensor, renderer=None, xaxis=\"\", yaxis=\"\", **kwargs):\n",
    "    px.line(utils.to_numpy(tensor), labels={\"x\":xaxis, \"y\":yaxis}, **kwargs).show(renderer)\n",
    "\n",
    "def scatter(x, y, xaxis=\"\", yaxis=\"\", caxis=\"\", renderer=None, **kwargs):\n",
    "    x = utils.to_numpy(x)\n",
    "    y = utils.to_numpy(y)\n",
    "    px.scatter(y=y, x=x, labels={\"x\":xaxis, \"y\":yaxis, \"color\":caxis}, **kwargs).show(renderer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "H4eoWAZQXXw-"
   },
   "outputs": [],
   "source": [
    "# Define the location to save the model, using a relative path\n",
    "PTH_LOCATION = \"workspace/_scratch/grokking_demo.pth\"\n",
    "\n",
    "# Create the directory if it does not exist\n",
    "os.makedirs(Path(PTH_LOCATION).parent, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W50vrZkBXXw-"
   },
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XgikoB4lXXw-"
   },
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "dVWqI8frXXw-"
   },
   "outputs": [],
   "source": [
    "p = 113\n",
    "frac_train = 0.3\n",
    "\n",
    "# Optimizer config\n",
    "lr = 1e-3\n",
    "wd = 1.\n",
    "betas = (0.9, 0.98)\n",
    "\n",
    "num_epochs = 25000\n",
    "checkpoint_every = 100\n",
    "\n",
    "DATA_SEED = 598"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L5l7DmSQXXw-"
   },
   "source": [
    "## Define Task\n",
    "* Define modular addition\n",
    "* Define the dataset & labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V0_cqPZoXXw_"
   },
   "source": [
    "Input format:\n",
    "|a|b|=|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "2Dgw3vyCXXw_"
   },
   "outputs": [],
   "source": [
    "a_vector = einops.repeat(torch.arange(p), \"i -> (i j)\", j=p)\n",
    "b_vector = einops.repeat(torch.arange(p), \"j -> (i j)\", i=p)\n",
    "equals_vector = einops.repeat(torch.tensor(113), \" -> (i j)\", i=p, j=p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "wReiwTfcXXw_",
    "outputId": "4307519b-c837-4739-bbe8-76e7a2f23a01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  0,   0, 113],\n",
      "        [  0,   1, 113],\n",
      "        [  0,   2, 113],\n",
      "        [  0,   3, 113],\n",
      "        [  0,   4, 113]], device='cuda:0')\n",
      "torch.Size([12769, 3])\n"
     ]
    }
   ],
   "source": [
    "dataset = torch.stack([a_vector, b_vector, equals_vector], dim=1).to(device)\n",
    "print(dataset[:5])\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "0iNRebVsXXw_",
    "outputId": "03c7f84e-a2da-49fd-adbc-b0db2a7bf1a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12769])\n",
      "tensor([0, 1, 2, 3, 4], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "labels = (dataset[:, 0] + dataset[:, 1]) % p\n",
    "print(labels.shape)\n",
    "print(labels[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E5QeD421XXw_"
   },
   "source": [
    "Convert this to a train + test set - 30% in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "jS_eWROjXXw_",
    "outputId": "34054229-d87a-4b27-b385-3f056ddb713c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 21,  31, 113],\n",
      "        [ 30,  98, 113],\n",
      "        [ 47,  10, 113],\n",
      "        [ 86,  21, 113],\n",
      "        [ 99,  83, 113]], device='cuda:0')\n",
      "tensor([ 52,  15,  57, 107,  69], device='cuda:0')\n",
      "torch.Size([3830, 3])\n",
      "tensor([[ 43,  40, 113],\n",
      "        [ 31,  42, 113],\n",
      "        [ 39,  63, 113],\n",
      "        [ 35,  61, 113],\n",
      "        [112, 102, 113]], device='cuda:0')\n",
      "tensor([ 83,  73, 102,  96, 101], device='cuda:0')\n",
      "torch.Size([8939, 3])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(DATA_SEED)\n",
    "indices = torch.randperm(p*p)\n",
    "cutoff = int(p*p*frac_train)\n",
    "train_indices = indices[:cutoff]\n",
    "test_indices = indices[cutoff:]\n",
    "\n",
    "train_data = dataset[train_indices]\n",
    "train_labels = labels[train_indices]\n",
    "test_data = dataset[test_indices]\n",
    "test_labels = labels[test_indices]\n",
    "print(train_data[:5])\n",
    "print(train_labels[:5])\n",
    "print(train_data.shape)\n",
    "print(test_data[:5])\n",
    "print(test_labels[:5])\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BH8CxJrdXXw_"
   },
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "hK1ycLBbXXxA"
   },
   "outputs": [],
   "source": [
    "\n",
    "cfg = HookedTransformerConfig(\n",
    "    n_layers = 1,\n",
    "    n_heads = 4,\n",
    "    d_model = 128,\n",
    "    d_head = 32,\n",
    "    d_mlp = 512,\n",
    "    act_fn = \"relu\",\n",
    "    normalization_type=None,\n",
    "    d_vocab=p+1,\n",
    "    d_vocab_out=p,\n",
    "    n_ctx=3,\n",
    "    init_weights=True,\n",
    "    device=device,\n",
    "    seed = 999,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "cWOi3f1_XXxA"
   },
   "outputs": [],
   "source": [
    "model = HookedTransformer(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7eOZaz3QXXxA"
   },
   "source": [
    "Disable the biases, as we don't need them for this task and it makes things easier to interpret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "wJRxaS3LXXxA"
   },
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if \"b_\" in name:\n",
    "        param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sT3xOQZpXXxA"
   },
   "source": [
    "## Define Optimizer + Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "vJUB8fl4XXxA"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd, betas=betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "v76P4xDtXXxA",
    "outputId": "f8944910-a9f5-4b11-a173-fc22fed76932"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.7359, device='cuda:0', dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "tensor(4.7330, device='cuda:0', dtype=torch.float64, grad_fn=<NegBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def loss_fn(logits, labels):\n",
    "    if len(logits.shape)==3:\n",
    "        logits = logits[:, -1]\n",
    "    logits = logits.to(torch.float64)\n",
    "    log_probs = logits.log_softmax(dim=-1)\n",
    "    correct_log_probs = log_probs.gather(dim=-1, index=labels[:, None])[:, 0]\n",
    "    return -correct_log_probs.mean()\n",
    "train_logits = model(train_data)\n",
    "train_loss = loss_fn(train_logits, train_labels)\n",
    "print(train_loss)\n",
    "test_logits = model(test_data)\n",
    "test_loss = loss_fn(test_logits, test_labels)\n",
    "print(test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "Oe0h91TeXXxA",
    "outputId": "727cdf78-e4bb-4ddc-8f36-5b88d74e8ece"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uniform loss:\n",
      "4.727387818712341\n"
     ]
    }
   ],
   "source": [
    "print(\"Uniform loss:\")\n",
    "print(np.log(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xr7HQIpgXXxB"
   },
   "source": [
    "## Actually Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5-n8l1ePXXxB"
   },
   "source": [
    "**Weird Decision:** Training the model with full batch training rather than stochastic gradient descent. We do this so to make training smoother and reduce the number of slingshots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "c477d3a6463646deb7cf1de99bb0199a"
     ]
    },
    "id": "ic4KslkCXXxB",
    "outputId": "dab1b58f-3bb2-4e48-e832-6e627ef84eaf"
   },
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "model_checkpoints = []\n",
    "checkpoint_epochs = []\n",
    "if TRAIN_MODEL:\n",
    "    for epoch in tqdm.tqdm(range(num_epochs)):\n",
    "        train_logits = model(train_data)\n",
    "        train_loss = loss_fn(train_logits, train_labels)\n",
    "        train_loss.backward()\n",
    "        train_losses.append(train_loss.item())\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            test_logits = model(test_data)\n",
    "            test_loss = loss_fn(test_logits, test_labels)\n",
    "            test_losses.append(test_loss.item())\n",
    "\n",
    "        if ((epoch+1)%checkpoint_every)==0:\n",
    "            checkpoint_epochs.append(epoch)\n",
    "            model_checkpoints.append(copy.deepcopy(model.state_dict()))\n",
    "            print(f\"Epoch {epoch} Train Loss {train_loss.item()} Test Loss {test_loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "pZ1UwFwjXXxB"
   },
   "outputs": [],
   "source": [
    "torch.save(\n",
    "    {\n",
    "        \"model\":model.state_dict(),\n",
    "        \"config\": model.cfg,\n",
    "        \"checkpoints\": model_checkpoints,\n",
    "        \"checkpoint_epochs\": checkpoint_epochs,\n",
    "        \"test_losses\": test_losses,\n",
    "        \"train_losses\": train_losses,\n",
    "        \"train_indices\": train_indices,\n",
    "        \"test_indices\": test_indices,\n",
    "    },\n",
    "    PTH_LOCATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "pStbjdgGXXxB"
   },
   "outputs": [],
   "source": [
    "if not TRAIN_MODEL:\n",
    "    cached_data = torch.load(PTH_LOCATION, weights_only=False)\n",
    "    model.load_state_dict(cached_data['model'])\n",
    "    model_checkpoints = cached_data[\"checkpoints\"]\n",
    "    checkpoint_epochs = cached_data[\"checkpoint_epochs\"]\n",
    "    test_losses = cached_data['test_losses']\n",
    "    train_losses = cached_data['train_losses']\n",
    "    train_indices = cached_data[\"train_indices\"]\n",
    "    test_indices = cached_data[\"test_indices\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tfZjJRmYXXxB"
   },
   "source": [
    "## Show Model Training Statistics, Check that it groks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "ABTV4hYEXXxB",
    "outputId": "8348da15-f56d-4680-9c69-8f3f503f70b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/burny/projects/ai/mechinterp/.venv/bin/python: No module named pip\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Mime type rendering requires nbformat>=4.2.0 but it is not installed",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[49]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m get_ipython().run_line_magic(\u001b[33m'\u001b[39m\u001b[33mpip\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33minstall git+https://github.com/neelnanda-io/neel-plotly.git\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mneel_plotly\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m line\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_losses\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m:\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_losses\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m:\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_losses\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxaxis\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mEpoch\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myaxis\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mLoss\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_y\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mTraining Curve for Modular Addition\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mline_labels\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtest\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoggle_x\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoggle_y\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/ai/mechinterp/.venv/lib/python3.12/site-packages/neel_plotly/plot.py:442\u001b[39m, in \u001b[36mline_or_scatter\u001b[39m\u001b[34m(y, x, mode, squeeze, plot_type, animation_frame, facet_col, **kwargs)\u001b[39m\n\u001b[32m    440\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m fig\n\u001b[32m    441\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m442\u001b[39m     \u001b[43mfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/ai/mechinterp/.venv/lib/python3.12/site-packages/plotly/basedatatypes.py:3420\u001b[39m, in \u001b[36mBaseFigure.show\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   3387\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3388\u001b[39m \u001b[33;03mShow a figure using either the default renderer(s) or the renderer(s)\u001b[39;00m\n\u001b[32m   3389\u001b[39m \u001b[33;03mspecified by the renderer argument\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   3416\u001b[39m \u001b[33;03mNone\u001b[39;00m\n\u001b[32m   3417\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3418\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplotly\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpio\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3420\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/ai/mechinterp/.venv/lib/python3.12/site-packages/plotly/io/_renderers.py:415\u001b[39m, in \u001b[36mshow\u001b[39m\u001b[34m(fig, renderer, validate, **kwargs)\u001b[39m\n\u001b[32m    410\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    411\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mMime type rendering requires ipython but it is not installed\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    412\u001b[39m     )\n\u001b[32m    414\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m nbformat \u001b[38;5;129;01mor\u001b[39;00m Version(nbformat.__version__) < Version(\u001b[33m\"\u001b[39m\u001b[33m4.2.0\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m415\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    416\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mMime type rendering requires nbformat>=4.2.0 but it is not installed\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    417\u001b[39m     )\n\u001b[32m    419\u001b[39m display_jupyter_version_warnings()\n\u001b[32m    421\u001b[39m ipython_display.display(bundle, raw=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mValueError\u001b[39m: Mime type rendering requires nbformat>=4.2.0 but it is not installed"
     ]
    }
   ],
   "source": [
    "%pip install git+https://github.com/neelnanda-io/neel-plotly.git\n",
    "from neel_plotly.plot import line\n",
    "line([train_losses[::100], test_losses[::100]], x=np.arange(0, len(train_losses), 100), xaxis=\"Epoch\", yaxis=\"Loss\", log_y=True, title=\"Training Curve for Modular Addition\", line_labels=['train', 'test'], toggle_x=True, toggle_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "71SYP9geXXxC"
   },
   "source": [
    "# Analysing the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WYpDQWlrXXxO"
   },
   "source": [
    "## Standard Things to Try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3UG5L2XXXXxO",
    "outputId": "0e87e17c-00f7-4c9b-ac60-4d32e530f802"
   },
   "outputs": [],
   "source": [
    "original_logits, cache = model.run_with_cache(dataset)\n",
    "print(original_logits.numel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o0OPqfnGXXxP"
   },
   "source": [
    "Get key weight matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4lPmy-NnXXxP",
    "outputId": "4321c8cd-7d3e-4436-f140-d8e6e722fdb0"
   },
   "outputs": [],
   "source": [
    "W_E = model.embed.W_E[:-1]\n",
    "print(\"W_E\", W_E.shape)\n",
    "W_neur = W_E @ model.blocks[0].attn.W_V @ model.blocks[0].attn.W_O @ model.blocks[0].mlp.W_in\n",
    "print(\"W_neur\", W_neur.shape)\n",
    "W_logit = model.blocks[0].mlp.W_out @ model.unembed.W_U\n",
    "print(\"W_logit\", W_logit.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VTQSqNlxXXxP",
    "outputId": "35298c6f-5f6b-4e7a-e723-a15f003a7782"
   },
   "outputs": [],
   "source": [
    "original_loss = loss_fn(original_logits, labels).item()\n",
    "print(\"Original Loss:\", original_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tEzOk2KDXXxP"
   },
   "source": [
    "### Looking at Activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sxN1bzsxXXxP"
   },
   "source": [
    "Helper variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Xt6UFXCXXxP"
   },
   "outputs": [],
   "source": [
    "pattern_a = cache[\"pattern\", 0, \"attn\"][:, :, -1, 0]\n",
    "pattern_b = cache[\"pattern\", 0, \"attn\"][:, :, -1, 1]\n",
    "neuron_acts = cache[\"post\", 0, \"mlp\"][:, -1, :]\n",
    "neuron_pre_acts = cache[\"pre\", 0, \"mlp\"][:, -1, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Il1bsZ5XXxQ"
   },
   "source": [
    "Get all shapes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZaX0pnJqXXxQ",
    "outputId": "f34c868d-e260-42c0-a9ce-d0c5bc1ed95f"
   },
   "outputs": [],
   "source": [
    "for param_name, param in cache.items():\n",
    "    print(param_name, param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_7oaHHDFXXxQ",
    "outputId": "3b91c166-d67a-4b5e-b202-0f200ff5798c"
   },
   "outputs": [],
   "source": [
    "imshow(cache[\"pattern\", 0].mean(dim=0)[:, -1, :], title=\"Average Attention Pattern per Head\", xaxis=\"Source\", yaxis=\"Head\", x=['a', 'b', '='])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OvViQCBlXXxQ",
    "outputId": "e59bea1f-b092-4c9c-9e47-8189f9f1cd10"
   },
   "outputs": [],
   "source": [
    "imshow(cache[\"pattern\", 0][5][:, -1, :], title=\"Average Attention Pattern per Head\", xaxis=\"Source\", yaxis=\"Head\", x=['a', 'b', '='])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b09WgONsXXxQ",
    "outputId": "0d2c808c-cbe7-4842-ad57-a1a81cf552d6"
   },
   "outputs": [],
   "source": [
    "dataset[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UPQzvJryXXxQ",
    "outputId": "96aa8831-9122-4e06-b805-7986ee2d6bab"
   },
   "outputs": [],
   "source": [
    "imshow(cache[\"pattern\", 0][:, 0, -1, 0].reshape(p, p), title=\"Attention for Head 0 from a -> =\", xaxis=\"b\", yaxis=\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rtGUN50WXXxR",
    "outputId": "ca328b30-d317-4238-8e4c-b10ab230ca79"
   },
   "outputs": [],
   "source": [
    "imshow(\n",
    "    einops.rearrange(cache[\"pattern\", 0][:, :, -1, 0], \"(a b) head -> head a b\", a=p, b=p),\n",
    "    title=\"Attention for Head 0 from a -> =\", xaxis=\"b\", yaxis=\"a\", facet_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7BSoDvHrXXxR"
   },
   "source": [
    "Plotting neuron activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aD5QkaphXXxR",
    "outputId": "149bb13a-5719-4c2d-bbe0-c6e7f4b8dc7e"
   },
   "outputs": [],
   "source": [
    "cache[\"post\", 0, \"mlp\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bvqpm9vpXXxR",
    "outputId": "5356b09d-c73e-47cb-83ee-1c1869a68cdb"
   },
   "outputs": [],
   "source": [
    "imshow(\n",
    "    einops.rearrange(neuron_acts[:, :5], \"(a b) neuron -> neuron a b\", a=p, b=p),\n",
    "    title=\"First 5 neuron acts\", xaxis=\"b\", yaxis=\"a\", facet_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zh1TvQoPXXxR"
   },
   "source": [
    "### Singular Value Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0dMFnPxEXXxS",
    "outputId": "4267b900-90bf-46ef-cca0-3da06f790a06"
   },
   "outputs": [],
   "source": [
    "W_E.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_1gHD5ILXXxS",
    "outputId": "aa404fe4-f051-491a-99ff-8db8fb00a135"
   },
   "outputs": [],
   "source": [
    "U, S, Vh = torch.svd(W_E)\n",
    "line(S, title=\"Singular Values\")\n",
    "imshow(U, title=\"Principal Components on the Input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wh6vQ_4VXXxS",
    "outputId": "5d2f07f9-d904-4ec5-899f-d191940fbec9"
   },
   "outputs": [],
   "source": [
    "# Control - random Gaussian matrix\n",
    "U, S, Vh = torch.svd(torch.randn_like(W_E))\n",
    "line(S, title=\"Singular Values Random\")\n",
    "imshow(U, title=\"Principal Components Random\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JBszoTF_XXxS"
   },
   "source": [
    "## Explaining Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MEhrTQ8UXXxS"
   },
   "source": [
    "### Analyse the Embedding - It's a Lookup Table!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vxt7NzfdXXxS",
    "outputId": "7cdf8ef4-eff2-4eef-ea20-4086db031e12"
   },
   "outputs": [],
   "source": [
    "U, S, Vh = torch.svd(W_E)\n",
    "line(U[:, :8].T, title=\"Principal Components of the embedding\", xaxis=\"Input Vocabulary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7cQyDroUXXxS",
    "outputId": "f346c036-714a-45d8-e588-132bc64d255c"
   },
   "outputs": [],
   "source": [
    "fourier_basis = []\n",
    "fourier_basis_names = []\n",
    "fourier_basis.append(torch.ones(p))\n",
    "fourier_basis_names.append(\"Constant\")\n",
    "for freq in range(1, p//2+1):\n",
    "    fourier_basis.append(torch.sin(torch.arange(p)*2 * torch.pi * freq / p))\n",
    "    fourier_basis_names.append(f\"Sin {freq}\")\n",
    "    fourier_basis.append(torch.cos(torch.arange(p)*2 * torch.pi * freq / p))\n",
    "    fourier_basis_names.append(f\"Cos {freq}\")\n",
    "fourier_basis = torch.stack(fourier_basis, dim=0).to(device)\n",
    "fourier_basis = fourier_basis/fourier_basis.norm(dim=-1, keepdim=True)\n",
    "imshow(fourier_basis, xaxis=\"Input\", yaxis=\"Component\", y=fourier_basis_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PLTs-E_cXXxT",
    "outputId": "b78c7446-0a6b-40f0-ee56-ee3aa9c1386e"
   },
   "outputs": [],
   "source": [
    "line(fourier_basis[:8], xaxis=\"Input\", line_labels=fourier_basis_names[:8], title=\"First 8 Fourier Components\")\n",
    "line(fourier_basis[25:29], xaxis=\"Input\", line_labels=fourier_basis_names[25:29], title=\"Middle Fourier Components\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OKsugwRdXXxT",
    "outputId": "52a5d0b4-f2d4-4168-b73b-8f0750cbe6d5"
   },
   "outputs": [],
   "source": [
    "imshow(fourier_basis @ fourier_basis.T, title=\"All Fourier Vectors are Orthogonal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "28jFklQ_XXxT"
   },
   "source": [
    "### Analyse the Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YYRdNXALXXxT",
    "outputId": "2e7b9809-b79a-4c00-e1b2-471c289b2d34"
   },
   "outputs": [],
   "source": [
    "imshow(fourier_basis @ W_E, yaxis=\"Fourier Component\", xaxis=\"Residual Stream\", y=fourier_basis_names, title=\"Embedding in Fourier Basis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VsdfbASDXXxT",
    "outputId": "76036586-8091-4c86-87fd-789698d693a0"
   },
   "outputs": [],
   "source": [
    "line((fourier_basis @ W_E).norm(dim=-1), xaxis=\"Fourier Component\", x=fourier_basis_names, title=\"Norms of Embedding in Fourier Basis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3pVCsymgXXxT",
    "outputId": "73be02dc-da45-46d9-c8db-38b0146bf3bd"
   },
   "outputs": [],
   "source": [
    "key_freqs = [17, 25, 32, 47]\n",
    "key_freq_indices = [33, 34, 49, 50, 63, 64, 93, 94]\n",
    "fourier_embed = fourier_basis @ W_E\n",
    "key_fourier_embed = fourier_embed[key_freq_indices]\n",
    "print(\"key_fourier_embed\", key_fourier_embed.shape)\n",
    "imshow(key_fourier_embed @ key_fourier_embed.T, title=\"Dot Product of embedding of key Fourier Terms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fw_wsOQkXXxT"
   },
   "source": [
    "### Key Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1xi0MbEwXXxU",
    "outputId": "a3158786-d9d9-428f-98b5-faa27455e4b1"
   },
   "outputs": [],
   "source": [
    "line(fourier_basis[[34, 50, 64, 94]], title=\"Cos of key freqs\", line_labels=[34, 50, 64, 94])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VM9KXTeLXXxU",
    "outputId": "98b44cd3-2c4c-4369-8bc2-44535d816510"
   },
   "outputs": [],
   "source": [
    "line(fourier_basis[[34, 50, 64, 94]].mean(0), title=\"Constructive Interference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "__wIMwrRXXxU"
   },
   "source": [
    "## Analyse Neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gb-_EupCXXxU",
    "outputId": "838cf130-7a39-4575-c277-195876c3b32f"
   },
   "outputs": [],
   "source": [
    "imshow(\n",
    "    einops.rearrange(neuron_acts[:, :5], \"(a b) neuron -> neuron a b\", a=p, b=p),\n",
    "    title=\"First 5 neuron acts\", xaxis=\"b\", yaxis=\"a\", facet_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Iq9Rf3MIXXxU",
    "outputId": "c2d7949b-4aeb-4877-af38-6a0476145c34"
   },
   "outputs": [],
   "source": [
    "imshow(\n",
    "    einops.rearrange(neuron_acts[:, 0], \"(a b) -> a b\", a=p, b=p),\n",
    "    title=\"First neuron act\", xaxis=\"b\", yaxis=\"a\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ekbojGG3XXxV",
    "outputId": "e0eb86de-62d0-459e-8ac1-99d335173836"
   },
   "outputs": [],
   "source": [
    "imshow(fourier_basis[94][None, :] * fourier_basis[94][:, None], title=\"Cos 47a * cos 47b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y2Z01QwIXXxV",
    "outputId": "edee3630-7baa-4b0f-c3db-06dac73b1cf5"
   },
   "outputs": [],
   "source": [
    "imshow(fourier_basis[94][None, :] * fourier_basis[0][:, None], title=\"Cos 47a * const\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4ckobqv9XXxW",
    "outputId": "f180a483-1c78-4cef-ed1e-2b203f0dbbab"
   },
   "outputs": [],
   "source": [
    "imshow(fourier_basis @ neuron_acts[:, 0].reshape(p, p) @ fourier_basis.T, title=\"2D Fourier Transformer of neuron 0\", xaxis=\"b\", yaxis=\"a\", x=fourier_basis_names, y=fourier_basis_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GqLLIAnIXXxW",
    "outputId": "8473ae4e-2229-4532-daad-9261edf816e2"
   },
   "outputs": [],
   "source": [
    "imshow(fourier_basis @ neuron_acts[:, 5].reshape(p, p) @ fourier_basis.T, title=\"2D Fourier Transformer of neuron 5\", xaxis=\"b\", yaxis=\"a\", x=fourier_basis_names, y=fourier_basis_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OKQdnH_FXXxW",
    "outputId": "881630ea-8b94-467f-faf0-1a4c4b56e4a9"
   },
   "outputs": [],
   "source": [
    "imshow(fourier_basis @ torch.randn_like(neuron_acts[:, 0]).reshape(p, p) @ fourier_basis.T, title=\"2D Fourier Transformer of RANDOM\", xaxis=\"b\", yaxis=\"a\", x=fourier_basis_names, y=fourier_basis_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GjMCWSSFXXxW"
   },
   "source": [
    "### Neuron Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MDOiEK1cXXxW",
    "outputId": "7427eeb4-5ee2-45a5-8438-dbef47ec6f44"
   },
   "outputs": [],
   "source": [
    "fourier_neuron_acts = fourier_basis @ einops.rearrange(neuron_acts, \"(a b) neuron -> neuron a b\", a=p, b=p) @ fourier_basis.T\n",
    "# Center these by removing the mean - doesn't matter!\n",
    "fourier_neuron_acts[:, 0, 0] = 0.\n",
    "print(\"fourier_neuron_acts\", fourier_neuron_acts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ewxUwXWPXXxW",
    "outputId": "48735675-5b62-4ae0-a852-f0fc89b71bd4"
   },
   "outputs": [],
   "source": [
    "neuron_freq_norm = torch.zeros(p//2, model.cfg.d_mlp).to(device)\n",
    "for freq in range(0, p//2):\n",
    "    for x in [0, 2*(freq+1) - 1, 2*(freq+1)]:\n",
    "        for y in [0, 2*(freq+1) - 1, 2*(freq+1)]:\n",
    "            neuron_freq_norm[freq] += fourier_neuron_acts[:, x, y]**2\n",
    "neuron_freq_norm = neuron_freq_norm / fourier_neuron_acts.pow(2).sum(dim=[-1, -2])[None, :]\n",
    "imshow(neuron_freq_norm, xaxis=\"Neuron\", yaxis=\"Freq\", y=torch.arange(1, p//2+1), title=\"Neuron Frac Explained by Freq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WtOquENeXXxX",
    "outputId": "f2e43e53-2b50-4740-ce35-5d5b6c69fb69"
   },
   "outputs": [],
   "source": [
    "line(neuron_freq_norm.max(dim=0).values.sort().values, xaxis=\"Neuron\", title=\"Max Neuron Frac Explained over Freqs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LoYYpwDKXXxX"
   },
   "source": [
    "## Read Off the Neuron-Logit Weights to Interpret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DE4Z-levXXxX"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cnW1giKaXXxX",
    "outputId": "213cb56c-8757-4845-a635-c730ca009f1c"
   },
   "outputs": [],
   "source": [
    "W_logit = model.blocks[0].mlp.W_out @ model.unembed.W_U\n",
    "print(\"W_logit\", W_logit.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CFPr5HE7XXxX",
    "outputId": "8406502a-7dcd-4244-ca96-a7a0e2fa1925"
   },
   "outputs": [],
   "source": [
    "line((W_logit @ fourier_basis.T).norm(dim=0), x=fourier_basis_names, title=\"W_logit in the Fourier Basis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YI8m-4ToXXxX",
    "outputId": "e209be2d-3b2a-46a0-c36a-66522a8f2633"
   },
   "outputs": [],
   "source": [
    "neurons_17 = neuron_freq_norm[17-1]>0.85\n",
    "neurons_17.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gDuOUdgjXXxY",
    "outputId": "673bdfee-a1e2-4d89-9e6c-fbeb3546559c"
   },
   "outputs": [],
   "source": [
    "neurons_17.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SlWGIbvlXXxY",
    "outputId": "de059e3f-edc9-4ada-c305-7d97e49c8754"
   },
   "outputs": [],
   "source": [
    "line((W_logit[neurons_17] @ fourier_basis.T).norm(dim=0), x=fourier_basis_names, title=\"W_logit for freq 17 neurons in the Fourier Basis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1h77GBZOXXxY"
   },
   "source": [
    "Study sin 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-qaTqHJeXXxY",
    "outputId": "73d1293b-2498-44b1-974b-d4ce9e98389c"
   },
   "outputs": [],
   "source": [
    "freq = 17\n",
    "W_logit_fourier = W_logit @ fourier_basis\n",
    "neurons_sin_17 = W_logit_fourier[:, 2*freq-1]\n",
    "line(neurons_sin_17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pnPm8b_0XXxY",
    "outputId": "1abd7243-bbcd-48ff-fd4a-4cdce36bd21a"
   },
   "outputs": [],
   "source": [
    "neuron_acts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DwFXhQLYXXxY",
    "outputId": "ad79cdb6-4cd5-4ea6-df05-3d2636bf9f6a"
   },
   "outputs": [],
   "source": [
    "inputs_sin_17c = neuron_acts @ neurons_sin_17\n",
    "imshow(fourier_basis @ inputs_sin_17c.reshape(p, p) @ fourier_basis.T, title=\"Fourier Heatmap over inputs for sin17c\", x=fourier_basis_names, y=fourier_basis_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c2CrsVe5XXxY"
   },
   "source": [
    "# Black Box Methods + Progress Measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9qmp-EPPXXxZ"
   },
   "source": [
    "## Setup Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "umNo6e8IXXxZ"
   },
   "source": [
    "Code to plot embedding freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZAEoISx9XXxZ"
   },
   "outputs": [],
   "source": [
    "def embed_to_cos_sin(fourier_embed):\n",
    "    if len(fourier_embed.shape) == 1:\n",
    "        return torch.stack([fourier_embed[1::2], fourier_embed[2::2]])\n",
    "    else:\n",
    "        return torch.stack([fourier_embed[:, 1::2], fourier_embed[:, 2::2]], dim=1)\n",
    "\n",
    "from neel_plotly.plot import melt\n",
    "\n",
    "def plot_embed_bars(\n",
    "    fourier_embed,\n",
    "    title=\"Norm of embedding of each Fourier Component\",\n",
    "    return_fig=False,\n",
    "    **kwargs\n",
    "):\n",
    "    cos_sin_embed = embed_to_cos_sin(fourier_embed)\n",
    "    df = melt(cos_sin_embed)\n",
    "    # display(df)\n",
    "    group_labels = {0: \"sin\", 1: \"cos\"}\n",
    "    df[\"Trig\"] = df[\"0\"].map(lambda x: group_labels[x])\n",
    "    fig = px.bar(\n",
    "        df,\n",
    "        barmode=\"group\",\n",
    "        color=\"Trig\",\n",
    "        x=\"1\",\n",
    "        y=\"value\",\n",
    "        labels={\"1\": \"$w_k$\", \"value\": \"Norm\"},\n",
    "        title=title,\n",
    "        **kwargs\n",
    "    )\n",
    "    fig.update_layout(dict(legend_title=\"\"))\n",
    "\n",
    "    if return_fig:\n",
    "        return fig\n",
    "    else:\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4gb07NKiXXxZ"
   },
   "source": [
    "Code to test a tensor of edited logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rdyZwKlxXXxZ"
   },
   "outputs": [],
   "source": [
    "def test_logits(logits, bias_correction=False, original_logits=None, mode=\"all\"):\n",
    "    # Calculates cross entropy loss of logits representing a batch of all p^2\n",
    "    # possible inputs\n",
    "    # Batch dimension is assumed to be first\n",
    "    if logits.shape[1] == p * p:\n",
    "        logits = logits.T\n",
    "    if logits.shape == torch.Size([p * p, p + 1]):\n",
    "        logits = logits[:, :-1]\n",
    "    logits = logits.reshape(p * p, p)\n",
    "    if bias_correction:\n",
    "        # Applies bias correction - we correct for any missing bias terms,\n",
    "        # independent of the input, by centering the new logits along the batch\n",
    "        # dimension, and then adding the average original logits across all inputs\n",
    "        logits = (\n",
    "            einops.reduce(original_logits - logits, \"batch ... -> ...\", \"mean\") + logits\n",
    "        )\n",
    "    if mode == \"train\":\n",
    "        return loss_fn(logits[train_indices], labels[train_indices])\n",
    "    elif mode == \"test\":\n",
    "        return loss_fn(logits[test_indices], labels[test_indices])\n",
    "    elif mode == \"all\":\n",
    "        return loss_fn(logits, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MowQ-G-BXXxZ"
   },
   "source": [
    "Code to run a metric over every checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rdae6IGfXXxZ"
   },
   "outputs": [],
   "source": [
    "metric_cache = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BkynJ2_2XXxa"
   },
   "outputs": [],
   "source": [
    "def get_metrics(model, metric_cache, metric_fn, name, reset=False):\n",
    "    if reset or (name not in metric_cache) or (len(metric_cache[name]) == 0):\n",
    "        metric_cache[name] = []\n",
    "        for c, sd in enumerate(tqdm.tqdm((model_checkpoints))):\n",
    "            model.reset_hooks()\n",
    "            model.load_state_dict(sd)\n",
    "            out = metric_fn(model)\n",
    "            if type(out) == torch.Tensor:\n",
    "                out = utils.to_numpy(out)\n",
    "            metric_cache[name].append(out)\n",
    "        model.load_state_dict(model_checkpoints[-1])\n",
    "        try:\n",
    "            metric_cache[name] = torch.tensor(metric_cache[name])\n",
    "        except:\n",
    "            metric_cache[name] = torch.tensor(np.array(metric_cache[name]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pOZBO4NLXXxa"
   },
   "source": [
    "## Defining Progress Measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_baxx3V6XXxa"
   },
   "source": [
    "### Loss Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K1CCshhJXXxa"
   },
   "outputs": [],
   "source": [
    "memorization_end_epoch = 1500\n",
    "circuit_formation_end_epoch = 13300\n",
    "cleanup_end_epoch = 16600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Nn3mfuoXXxa"
   },
   "outputs": [],
   "source": [
    "def add_lines(figure):\n",
    "    figure.add_vline(memorization_end_epoch, line_dash=\"dash\", opacity=0.7)\n",
    "    figure.add_vline(circuit_formation_end_epoch, line_dash=\"dash\", opacity=0.7)\n",
    "    figure.add_vline(cleanup_end_epoch, line_dash=\"dash\", opacity=0.7)\n",
    "    return figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S3-A2bLcXXxa",
    "outputId": "387b589a-cc63-4160-db6e-46c06b4e5a8d"
   },
   "outputs": [],
   "source": [
    "fig = line([train_losses[::100], test_losses[::100]], x=np.arange(0, len(train_losses), 100), xaxis=\"Epoch\", yaxis=\"Loss\", log_y=True, title=\"Training Curve for Modular Addition\", line_labels=['train', 'test'], toggle_x=True, toggle_y=True, return_fig=True)\n",
    "add_lines(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ukaXtg9XXXxa"
   },
   "source": [
    "### Logit Periodicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QV-r7MZeXXxa",
    "outputId": "95df1ca8-2713-4e54-8255-180531af72af"
   },
   "outputs": [],
   "source": [
    "all_logits = original_logits[:, -1, :]\n",
    "print(all_logits.shape)\n",
    "all_logits = einops.rearrange(all_logits, \"(a b) c -> a b c\", a=p, b=p)\n",
    "print(all_logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WPIB1X1mXXxb",
    "outputId": "54bf57a0-8989-47dd-f2fc-a5869a954ac4"
   },
   "outputs": [],
   "source": [
    "coses = {}\n",
    "for freq in key_freqs:\n",
    "    print(\"Freq:\", freq)\n",
    "    a = torch.arange(p)[:, None, None]\n",
    "    b = torch.arange(p)[None, :, None]\n",
    "    c = torch.arange(p)[None, None, :]\n",
    "    cube_predicted_logits = torch.cos(freq * 2 * torch.pi / p * (a + b - c)).to(device)\n",
    "    cube_predicted_logits /= cube_predicted_logits.norm()\n",
    "    coses[freq] = cube_predicted_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8wYyBxKzXXxb",
    "outputId": "fbc0a723-2e0b-48e0-e6b6-e0dd8187dedc"
   },
   "outputs": [],
   "source": [
    "approximated_logits = torch.zeros_like(all_logits)\n",
    "for freq in key_freqs:\n",
    "    print(\"Freq:\", freq)\n",
    "    coeff = (all_logits * coses[freq]).sum()\n",
    "    print(\"Coeff:\", coeff)\n",
    "    cosine_sim = coeff / all_logits.norm()\n",
    "    print(\"Cosine Sim:\", cosine_sim)\n",
    "    approximated_logits += coeff * coses[freq]\n",
    "residual = all_logits - approximated_logits\n",
    "print(\"Residual size:\", residual.norm())\n",
    "print(\"Residual fraction of norm:\", residual.norm()/all_logits.norm())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QPICe26oXXxb",
    "outputId": "cb759986-82c2-45ff-a405-3ed6e6da4610"
   },
   "outputs": [],
   "source": [
    "random_logit_cube = torch.randn_like(all_logits)\n",
    "print((all_logits * random_logit_cube).sum()/random_logit_cube.norm()/all_logits.norm())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-X5uhCAQXXxb",
    "outputId": "fb6a72d1-8f41-4bcc-a344-f29a24907575"
   },
   "outputs": [],
   "source": [
    "test_logits(all_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ADW4Yy7sXXxb",
    "outputId": "f769e119-e2a2-4ef6-fc36-39102800beff"
   },
   "outputs": [],
   "source": [
    "test_logits(approximated_logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kUMKVIXiXXxc"
   },
   "source": [
    "#### Look During Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SmyetzsqXXxc",
    "outputId": "671ee608-c94f-46a8-f408-99fca48e7354"
   },
   "outputs": [],
   "source": [
    "cos_cube = []\n",
    "for freq in range(1, p//2 + 1):\n",
    "    a = torch.arange(p)[:, None, None]\n",
    "    b = torch.arange(p)[None, :, None]\n",
    "    c = torch.arange(p)[None, None, :]\n",
    "    cube_predicted_logits = torch.cos(freq * 2 * torch.pi / p * (a + b - c)).to(device)\n",
    "    cube_predicted_logits /= cube_predicted_logits.norm()\n",
    "    cos_cube.append(cube_predicted_logits)\n",
    "cos_cube = torch.stack(cos_cube, dim=0)\n",
    "print(cos_cube.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "4d3444ac0c09485bb2297c35246614f4"
     ]
    },
    "id": "KYc5UtEaXXxc",
    "outputId": "0966c13f-5e63-43b5-9e60-5c20c59bd078"
   },
   "outputs": [],
   "source": [
    "def get_cos_coeffs(model):\n",
    "    logits = model(dataset)[:, -1]\n",
    "    logits = einops.rearrange(logits, \"(a b) c -> a b c\", a=p, b=p)\n",
    "    vals = (cos_cube * logits[None, :, :, :]).sum([-3, -2, -1])\n",
    "    return vals\n",
    "\n",
    "\n",
    "get_metrics(model, metric_cache, get_cos_coeffs, \"cos_coeffs\")\n",
    "print(metric_cache[\"cos_coeffs\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HC4imJHuXXxc",
    "outputId": "8f8dae8c-a70f-45fe-a5de-e4c08609e54f"
   },
   "outputs": [],
   "source": [
    "fig = line(metric_cache[\"cos_coeffs\"].T, line_labels=[f\"Freq {i}\" for i in range(1, p//2+1)], title=\"Coefficients with Predicted Logits\", xaxis=\"Epoch\", x=checkpoint_epochs, yaxis=\"Coefficient\", return_fig=True)\n",
    "add_lines(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "64c4305b8e8f466f8dd9240817fbc5e2"
     ]
    },
    "id": "PcZsdK6mXXxc",
    "outputId": "d1e4dacd-a877-4fd5-8052-4909329a721c"
   },
   "outputs": [],
   "source": [
    "def get_cos_sim(model):\n",
    "    logits = model(dataset)[:, -1]\n",
    "    logits = einops.rearrange(logits, \"(a b) c -> a b c\", a=p, b=p)\n",
    "    vals = (cos_cube * logits[None, :, :, :]).sum([-3, -2, -1])\n",
    "    return vals / logits.norm()\n",
    "\n",
    "get_metrics(model, metric_cache, get_cos_sim, \"cos_sim\") # You may need a big GPU. If you don't have one and can't work around this, raise an issue for help!\n",
    "print(metric_cache[\"cos_sim\"].shape)\n",
    "\n",
    "fig = line(metric_cache[\"cos_sim\"].T, line_labels=[f\"Freq {i}\" for i in range(1, p//2+1)], title=\"Cosine Sim with Predicted Logits\", xaxis=\"Epoch\", x=checkpoint_epochs, yaxis=\"Cosine Sim\", return_fig=True)\n",
    "add_lines(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "93e129d45acf43d29dc422409440ad70"
     ]
    },
    "id": "iYSKe48CXXxd",
    "outputId": "06807b18-b0a5-4c3e-d101-b93bfc562ea2"
   },
   "outputs": [],
   "source": [
    "def get_residual_cos_sim(model):\n",
    "    logits = model(dataset)[:, -1]\n",
    "    logits = einops.rearrange(logits, \"(a b) c -> a b c\", a=p, b=p)\n",
    "    vals = (cos_cube * logits[None, :, :, :]).sum([-3, -2, -1])\n",
    "    residual = logits - (vals[:, None, None, None] * cos_cube).sum(dim=0)\n",
    "    return residual.norm() / logits.norm()\n",
    "\n",
    "get_metrics(model, metric_cache, get_residual_cos_sim, \"residual_cos_sim\")\n",
    "print(metric_cache[\"residual_cos_sim\"].shape)\n",
    "\n",
    "fig = line([metric_cache[\"cos_sim\"][:, i] for i in range(p//2)]+[metric_cache[\"residual_cos_sim\"]], line_labels=[f\"Freq {i}\" for i in range(1, p//2+1)]+[\"residual\"], title=\"Cosine Sim with Predicted Logits + Residual\", xaxis=\"Epoch\", x=checkpoint_epochs, yaxis=\"Cosine Sim\", return_fig=True)\n",
    "add_lines(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PFW7vfEUXXxd"
   },
   "source": [
    "## Restricted Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nK8hjS_XXXxd",
    "outputId": "1ffdd116-9915-4a0e-d308-53aca59e4e80"
   },
   "outputs": [],
   "source": [
    "neuron_acts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "72LQPoQYXXxe",
    "outputId": "1e9e733d-7848-48fe-c098-b70e3fb6b9b1"
   },
   "outputs": [],
   "source": [
    "neuron_acts_square = einops.rearrange(neuron_acts, \"(a b) neur -> a b neur\", a=p, b=p).clone()\n",
    "# Center it\n",
    "neuron_acts_square -= einops.reduce(neuron_acts_square, \"a b neur -> 1 1 neur\", \"mean\")\n",
    "neuron_acts_square_fourier = einsum(\"a b neur, fa a, fb b -> fa fb neur\", neuron_acts_square, fourier_basis, fourier_basis)\n",
    "imshow(neuron_acts_square_fourier.norm(dim=-1), xaxis=\"Fourier Component b\", yaxis=\"Fourier Component a\", title=\"Norms of neuron activations by Fourier Component\", x=fourier_basis_names, y=fourier_basis_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y5cTjE4kXXxe",
    "outputId": "841fd4db-8852-49cf-ce28-240935ab0152"
   },
   "outputs": [],
   "source": [
    "original_logits, cache = model.run_with_cache(dataset)\n",
    "print(original_logits.numel())\n",
    "neuron_acts = cache[\"post\", 0, \"mlp\"][:, -1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tQgIdg7jXXxe",
    "outputId": "e92ebb1e-f178-40e4-967a-245933e2300d"
   },
   "outputs": [],
   "source": [
    "approx_neuron_acts = torch.zeros_like(neuron_acts)\n",
    "approx_neuron_acts += neuron_acts.mean(dim=0)\n",
    "a = torch.arange(p)[:, None]\n",
    "b = torch.arange(p)[None, :]\n",
    "for freq in key_freqs:\n",
    "    cos_apb_vec = torch.cos(freq * 2 * torch.pi / p * (a + b)).to(device)\n",
    "    cos_apb_vec /= cos_apb_vec.norm()\n",
    "    cos_apb_vec = einops.rearrange(cos_apb_vec, \"a b -> (a b) 1\")\n",
    "    approx_neuron_acts += (neuron_acts * cos_apb_vec).sum(dim=0) * cos_apb_vec\n",
    "    sin_apb_vec = torch.sin(freq * 2 * torch.pi / p * (a + b)).to(device)\n",
    "    sin_apb_vec /= sin_apb_vec.norm()\n",
    "    sin_apb_vec = einops.rearrange(sin_apb_vec, \"a b -> (a b) 1\")\n",
    "    approx_neuron_acts += (neuron_acts * sin_apb_vec).sum(dim=0) * sin_apb_vec\n",
    "restricted_logits = approx_neuron_acts @ W_logit\n",
    "print(loss_fn(restricted_logits[test_indices], test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nps_Sr5uXXxe",
    "outputId": "c6217287-3adb-4ea8-e87d-c62e8acf9304"
   },
   "outputs": [],
   "source": [
    "print(loss_fn(all_logits, labels)) # This bugged on models not fully trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x5JTWnL6XXxe"
   },
   "source": [
    "### Look During Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iv_J5dt2XXxf",
    "outputId": "9bd3328f-5d35-4cfe-dc50-0b1b9cbe68c6"
   },
   "outputs": [],
   "source": [
    "def get_restricted_loss(model):\n",
    "    logits, cache = model.run_with_cache(dataset)\n",
    "    logits = logits[:, -1, :]\n",
    "    neuron_acts = cache[\"post\", 0, \"mlp\"][:, -1, :]\n",
    "    approx_neuron_acts = torch.zeros_like(neuron_acts)\n",
    "    approx_neuron_acts += neuron_acts.mean(dim=0)\n",
    "    a = torch.arange(p)[:, None]\n",
    "    b = torch.arange(p)[None, :]\n",
    "    for freq in key_freqs:\n",
    "        cos_apb_vec = torch.cos(freq * 2 * torch.pi / p * (a + b)).to(device)\n",
    "        cos_apb_vec /= cos_apb_vec.norm()\n",
    "        cos_apb_vec = einops.rearrange(cos_apb_vec, \"a b -> (a b) 1\")\n",
    "        approx_neuron_acts += (neuron_acts * cos_apb_vec).sum(dim=0) * cos_apb_vec\n",
    "        sin_apb_vec = torch.sin(freq * 2 * torch.pi / p * (a + b)).to(device)\n",
    "        sin_apb_vec /= sin_apb_vec.norm()\n",
    "        sin_apb_vec = einops.rearrange(sin_apb_vec, \"a b -> (a b) 1\")\n",
    "        approx_neuron_acts += (neuron_acts * sin_apb_vec).sum(dim=0) * sin_apb_vec\n",
    "    restricted_logits = approx_neuron_acts @ model.blocks[0].mlp.W_out @ model.unembed.W_U\n",
    "    # Add bias term\n",
    "    restricted_logits += logits.mean(dim=0, keepdim=True) - restricted_logits.mean(dim=0, keepdim=True)\n",
    "    return loss_fn(restricted_logits[test_indices], test_labels)\n",
    "get_restricted_loss(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "3079070c7f5445aabbc6a198a53b04b2"
     ]
    },
    "id": "x4Yqxi39XXxf",
    "outputId": "6d2782a5-6294-48f4-811b-3e9de9d71b35"
   },
   "outputs": [],
   "source": [
    "get_metrics(model, metric_cache, get_restricted_loss, \"restricted_loss\", reset=True)\n",
    "print(metric_cache[\"restricted_loss\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UHWQECEzXXxf",
    "outputId": "b9ee17f6-aea9-448c-c6f1-86993e1b1fbf"
   },
   "outputs": [],
   "source": [
    "fig = line([train_losses[::100], test_losses[::100], metric_cache[\"restricted_loss\"]], x=np.arange(0, len(train_losses), 100), xaxis=\"Epoch\", yaxis=\"Loss\", log_y=True, title=\"Restricted Loss Curve\", line_labels=['train', 'test', \"restricted_loss\"], toggle_x=True, toggle_y=True, return_fig=True)\n",
    "add_lines(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "krHebo_hXXxf",
    "outputId": "20042939-fb97-404f-94af-f6b716516ec3"
   },
   "outputs": [],
   "source": [
    "fig = line([torch.tensor(test_losses[::100])/metric_cache[\"restricted_loss\"]], x=np.arange(0, len(train_losses), 100), xaxis=\"Epoch\", yaxis=\"Loss\", log_y=True, title=\"Restricted Loss to Test Loss Ratio\", toggle_x=True, toggle_y=True, return_fig=True)\n",
    "# WARNING: bugged when cancelling training half way thr ough\n",
    "add_lines(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AqeA7sSoXXxf"
   },
   "source": [
    "## Excluded Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bbs88a4-XXxf",
    "outputId": "e48d1386-4365-418d-e830-eeb1de965a84"
   },
   "outputs": [],
   "source": [
    "approx_neuron_acts = torch.zeros_like(neuron_acts)\n",
    "# approx_neuron_acts += neuron_acts.mean(dim=0)\n",
    "a = torch.arange(p)[:, None]\n",
    "b = torch.arange(p)[None, :]\n",
    "for freq in key_freqs:\n",
    "    cos_apb_vec = torch.cos(freq * 2 * torch.pi / p * (a + b)).to(device)\n",
    "    cos_apb_vec /= cos_apb_vec.norm()\n",
    "    cos_apb_vec = einops.rearrange(cos_apb_vec, \"a b -> (a b) 1\")\n",
    "    approx_neuron_acts += (neuron_acts * cos_apb_vec).sum(dim=0) * cos_apb_vec\n",
    "    sin_apb_vec = torch.sin(freq * 2 * torch.pi / p * (a + b)).to(device)\n",
    "    sin_apb_vec /= sin_apb_vec.norm()\n",
    "    sin_apb_vec = einops.rearrange(sin_apb_vec, \"a b -> (a b) 1\")\n",
    "    approx_neuron_acts += (neuron_acts * sin_apb_vec).sum(dim=0) * sin_apb_vec\n",
    "excluded_neuron_acts = neuron_acts - approx_neuron_acts\n",
    "excluded_logits = excluded_neuron_acts @ W_logit\n",
    "print(loss_fn(excluded_logits[train_indices], train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1OcgV52CXXxf",
    "outputId": "57a484ff-4609-40bd-bc3a-7f5a38f27afa"
   },
   "outputs": [],
   "source": [
    "def get_excluded_loss(model):\n",
    "    logits, cache = model.run_with_cache(dataset)\n",
    "    logits = logits[:, -1, :]\n",
    "    neuron_acts = cache[\"post\", 0, \"mlp\"][:, -1, :]\n",
    "    approx_neuron_acts = torch.zeros_like(neuron_acts)\n",
    "    # approx_neuron_acts += neuron_acts.mean(dim=0)\n",
    "    a = torch.arange(p)[:, None]\n",
    "    b = torch.arange(p)[None, :]\n",
    "    for freq in key_freqs:\n",
    "        cos_apb_vec = torch.cos(freq * 2 * torch.pi / p * (a + b)).to(device)\n",
    "        cos_apb_vec /= cos_apb_vec.norm()\n",
    "        cos_apb_vec = einops.rearrange(cos_apb_vec, \"a b -> (a b) 1\")\n",
    "        approx_neuron_acts += (neuron_acts * cos_apb_vec).sum(dim=0) * cos_apb_vec\n",
    "        sin_apb_vec = torch.sin(freq * 2 * torch.pi / p * (a + b)).to(device)\n",
    "        sin_apb_vec /= sin_apb_vec.norm()\n",
    "        sin_apb_vec = einops.rearrange(sin_apb_vec, \"a b -> (a b) 1\")\n",
    "        approx_neuron_acts += (neuron_acts * sin_apb_vec).sum(dim=0) * sin_apb_vec\n",
    "    excluded_neuron_acts = neuron_acts - approx_neuron_acts\n",
    "    residual_stream_final = excluded_neuron_acts @ model.blocks[0].mlp.W_out + cache[\"resid_mid\", 0][:, -1, :]\n",
    "    excluded_logits = residual_stream_final @ model.unembed.W_U\n",
    "    return loss_fn(excluded_logits[train_indices], train_labels)\n",
    "get_excluded_loss(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "3661a3612a0744aa806d8f7c2e884666"
     ]
    },
    "id": "lutfVX03XXxg",
    "outputId": "bd5990f2-cf40-4caa-ca96-84fcac655b51"
   },
   "outputs": [],
   "source": [
    "get_metrics(model, metric_cache, get_excluded_loss, \"excluded_loss\", reset=True)\n",
    "print(metric_cache[\"excluded_loss\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hiLuc14PXXxg",
    "outputId": "43ca801c-7ba4-46ad-f1b7-6f3ec546050d"
   },
   "outputs": [],
   "source": [
    "fig = line([train_losses[::100], test_losses[::100], metric_cache[\"excluded_loss\"], metric_cache[\"restricted_loss\"]], x=np.arange(0, len(train_losses), 100), xaxis=\"Epoch\", yaxis=\"Loss\", log_y=True, title=\"Excluded and Restricted Loss Curve\", line_labels=['train', 'test', \"excluded_loss\", \"restricted_loss\"], toggle_x=True, toggle_y=True, return_fig=True)\n",
    "\n",
    "add_lines(fig)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
